{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d954819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import models\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84d624bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_dir_path = 'dataset/2afc/train/mix/ref/'\n",
    "p0_dir_path = 'dataset/2afc/train/mix/p0/'\n",
    "p1_dir_path = 'dataset/2afc/train/mix/p1/'\n",
    "judge_dir_path = 'dataset/2afc/train/mix/judge/'\n",
    "\n",
    "img_size = 64\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e90b1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoAFCDataset(Dataset):\n",
    "    def __init__(self, ref_dir, p0_dir, p1_dir, judge_dir, transform=None):\n",
    "\n",
    "        self.ref_paths = sorted(get_paths(ref_dir, mode='img'))\n",
    "        self.p0_paths = sorted(get_paths(p0_dir, mode='img'))\n",
    "        self.p1_paths = sorted(get_paths(p1_dir, mode='img'))\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        self.judge_paths = sorted(get_paths(judge_dir, mode='np'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ref_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        p0_img = Image.open(self.p0_paths[idx]).convert('RGB')\n",
    "        p1_img = Image.open(self.p1_paths[idx]).convert('RGB')\n",
    "        ref_img = Image.open(self.ref_paths[idx]).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            p0_img = self.transform(p0_img)\n",
    "            p1_img = self.transform(p1_img)\n",
    "            ref_img = self.transform(ref_img)\n",
    "\n",
    "        judge_img = np.load(self.judge_paths[idx])\n",
    "\n",
    "        return {'p0': p0_img, 'p1': p1_img, 'ref': ref_img, 'judge': judge_img}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "722fc64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_EXTENSIONS = [\n",
    "    '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
    "    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n",
    "]\n",
    "\n",
    "NP_EXTENSIONS = ['.npy', ]\n",
    "\n",
    "\n",
    "def get_paths(dir_path, mode='img'):\n",
    "    paths = []\n",
    "    for root, _, filenames in sorted(os.walk(dir_path)):\n",
    "        for filename in filenames:\n",
    "            if is_right_file(filename, mode=mode):\n",
    "                path = os.path.join(root, filename)\n",
    "                paths.append(path)\n",
    "    return paths\n",
    "\n",
    "\n",
    "def is_right_file(filename, mode='img'):\n",
    "    if mode == 'img':\n",
    "        return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "    else:\n",
    "        return any(filename.endswith(extension) for extension in NP_EXTENSIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7874aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "dataset = TwoAFCDataset(ref_dir_path,\n",
    "                        p0_dir_path,\n",
    "                        p1_dir_path,\n",
    "                        judge_dir_path,\n",
    "                        transform=data_transform)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc58531",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self, requires_grad=False, pretrained=True):\n",
    "        super(VGG, self).__init__()\n",
    "        vgg_pretrained_features = models.vgg16(pretrained=pretrained).features\n",
    "        self.slice1 = torch.nn.Sequential()\n",
    "        self.slice2 = torch.nn.Sequential()\n",
    "        self.slice3 = torch.nn.Sequential()\n",
    "        self.slice4 = torch.nn.Sequential()\n",
    "        self.slice5 = torch.nn.Sequential()\n",
    "        self.N_slices = 5\n",
    "        for x in range(4):\n",
    "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(4, 9):\n",
    "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(9, 16):\n",
    "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(16, 23):\n",
    "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(23, 30):\n",
    "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
    "        if not requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.slice1(x)\n",
    "        h_relu1_2 = h\n",
    "        h = self.slice2(h)\n",
    "        h_relu2_2 = h\n",
    "        h = self.slice3(h)\n",
    "        h_relu3_3 = h\n",
    "        h = self.slice4(h)\n",
    "        h_relu4_3 = h\n",
    "        h = self.slice5(h)\n",
    "        h_relu5_3 = h\n",
    "        # noinspection PyTypeChecker\n",
    "        vgg_outputs = namedtuple('VggOutputs', ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3'])\n",
    "        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18eb643",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(nn.Module):\n",
    "    \"\"\" A single linear layer which does a 1x1 conv \"\"\"\n",
    "\n",
    "    def __init__(self, chn_in, chn_out=1, use_dropout=False):\n",
    "        super(LinearLayer, self).__init__()\n",
    "\n",
    "        layers = [nn.Dropout(), ] if use_dropout else []\n",
    "        layers += [nn.Conv2d(chn_in, chn_out, kernel_size=(1, 1), stride=(1, 1), padding=0, bias=False), ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def normalize_tensor(in_feat, eps=1e-10):\n",
    "    norm_factor = torch.sqrt(torch.sum(in_feat ** 2, dim=1, keepdim=True))\n",
    "    return in_feat / (norm_factor + eps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LPIPS(nn.Module):\n",
    "    def __init__(self, pretrained=True, net='alex', spatial=False, pnet_rand=False, pnet_tune=False, use_dropout=True,\n",
    "                 model_path=None, eval_mode=True, verbose=True):\n",
    "        # lpips - [True] means with linear calibration on top of base network\n",
    "        # pretrained - [True] means load linear weights\n",
    "\n",
    "        super(LPIPS, self).__init__()\n",
    "\n",
    "        self.pnet_type = net\n",
    "        self.pnet_tune = pnet_tune\n",
    "        self.pnet_rand = pnet_rand\n",
    "        self.spatial = spatial\n",
    "\n",
    "        self.chns = [64, 128, 256, 512, 512]\n",
    "\n",
    "        self.L = len(self.chns)\n",
    "\n",
    "        self.net = VGG(pretrained=not self.pnet_rand, requires_grad=self.pnet_tune)\n",
    "\n",
    "        self.lin0 = LinearLayer(self.chns[0], use_dropout=use_dropout)\n",
    "        self.lin1 = LinearLayer(self.chns[1], use_dropout=use_dropout)\n",
    "        self.lin2 = LinearLayer(self.chns[2], use_dropout=use_dropout)\n",
    "        self.lin3 = LinearLayer(self.chns[3], use_dropout=use_dropout)\n",
    "        self.lin4 = LinearLayer(self.chns[4], use_dropout=use_dropout)\n",
    "        self.lins = [self.lin0, self.lin1, self.lin2, self.lin3, self.lin4]\n",
    "\n",
    "        if pretrained:\n",
    "            if model_path is None:\n",
    "                import inspect\n",
    "                import os\n",
    "                model_path = os.path.abspath(\n",
    "                    os.path.join(inspect.getfile(self.__init__), '..', 'weights/v%s/%s.pth' % (version, net)))\n",
    "\n",
    "            if verbose:\n",
    "                print('Loading model from: %s' % model_path)\n",
    "            self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n",
    "\n",
    "        if eval_mode:\n",
    "            self.eval()\n",
    "\n",
    "    def forward(self, in0, in1, retPerLayer=False, normalize=False):\n",
    "        # turn on this flag if input is [0,1] so it can be adjusted to [-1, +1]\n",
    "        if normalize:\n",
    "            in0 = 2 * in0 - 1\n",
    "            in1 = 2 * in1 - 1\n",
    "\n",
    "        in0_input, in1_input = (in0, in1)\n",
    "        outs0, outs1 = self.net.forward(in0_input), self.net.forward(in1_input)\n",
    "        feats0, feats1, diffs = {}, {}, {}\n",
    "\n",
    "        for kk in range(self.L):\n",
    "            feats0[kk], feats1[kk] = normalize_tensor(outs0[kk]), normalize_tensor(outs1[kk])\n",
    "            diffs[kk] = (feats0[kk] - feats1[kk]) ** 2\n",
    "\n",
    "        if self.spatial:\n",
    "            res = [upsample(self.lins[kk](diffs[kk]), out_HW=in0.shape[2:]) for kk in range(self.L)]\n",
    "        else:\n",
    "            res = [spatial_average(self.lins[kk](diffs[kk]), keepdim=True) for kk in range(self.L)]\n",
    "\n",
    "        val = res[0]\n",
    "        for l in range(1, self.L):\n",
    "            val += res[l]\n",
    "\n",
    "        if retPerLayer:\n",
    "            return val, res\n",
    "        else:\n",
    "            return val"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b21b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lpips]",
   "language": "python",
   "name": "conda-env-lpips-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}