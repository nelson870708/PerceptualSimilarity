{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d954819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import transforms, models\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84d624bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_dir_path = 'dataset/2afc/train/mix/ref/'\n",
    "p0_dir_path = 'dataset/2afc/train/mix/p0/'\n",
    "p1_dir_path = 'dataset/2afc/train/mix/p1/'\n",
    "judge_dir_path = 'dataset/2afc/train/mix/judge/'\n",
    "\n",
    "img_size = 64\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e90b1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoAFCDataset(Dataset):\n",
    "    def __init__(self, ref_dir, p0_dir, p1_dir, judge_dir, transform=None):\n",
    "        \n",
    "        self.ref_paths = sorted(get_paths(ref_dir, mode='img'))\n",
    "        self.p0_paths = sorted(get_paths(p0_dir, mode='img'))\n",
    "        self.p1_paths = sorted(get_paths(p1_dir, mode='img'))\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        self.judge_paths = sorted(get_paths(judge_dir, mode='np'))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ref_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        p0_img = Image.open(self.p0_paths[idx]).convert('RGB')\n",
    "        p1_img = Image.open(self.p1_paths[idx]).convert('RGB')\n",
    "        ref_img = Image.open(self.ref_paths[idx]).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            p0_img = self.transform(p0_img)\n",
    "            p1_img = self.transform(p1_img)\n",
    "            ref_img = self.transform(ref_img)\n",
    "        \n",
    "        judge_img = np.load(self.judge_paths[idx])\n",
    "        \n",
    "        return {'p0': p0_img, 'p1': p1_img, 'ref': ref_img, 'judge': judge_img}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "722fc64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_EXTENSIONS = [\n",
    "    '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
    "    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n",
    "]\n",
    "\n",
    "NP_EXTENSIONS = ['.npy', ]\n",
    "\n",
    "def get_paths(dir_path, mode='img'):\n",
    "    paths = []\n",
    "    for root, _, filenames in sorted(os.walk(dir_path)):\n",
    "        for filename in filenames:\n",
    "            if is_right_file(filename, mode=mode):\n",
    "                path = os.path.join(root, filename)\n",
    "                paths.append(path)\n",
    "    return paths\n",
    "\n",
    "def is_right_file(filename, mode='img'):\n",
    "    if mode == 'img':\n",
    "        return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "    else:\n",
    "        return any(filename.endswith(extension) for extension in NP_EXTENSIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7874aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "dataset = TwoAFCDataset(ref_dir_path, \n",
    "                        p0_dir_path,\n",
    "                        p1_dir_path,\n",
    "                        judge_dir_path,\n",
    "                        transform=data_transform)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,            \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc58531",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(torch.nn.Module):\n",
    "    def __init__(self, requires_grad=False, pretrained=True):\n",
    "        super(VGG16, self).__init__()\n",
    "        vgg_pretrained_features = models.vgg16(pretrained=pretrained).features\n",
    "        self.slice1 = torch.nn.Sequential()\n",
    "        self.slice2 = torch.nn.Sequential()\n",
    "        self.slice3 = torch.nn.Sequential()\n",
    "        self.slice4 = torch.nn.Sequential()\n",
    "        self.slice5 = torch.nn.Sequential()\n",
    "        self.N_slices = 5\n",
    "        for x in range(4):\n",
    "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(4, 9):\n",
    "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(9, 16):\n",
    "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(16, 23):\n",
    "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(23, 30):\n",
    "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
    "        if not requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.slice1(x)\n",
    "        h_relu1_2 = h\n",
    "        h = self.slice2(h)\n",
    "        h_relu2_2 = h\n",
    "        h = self.slice3(h)\n",
    "        h_relu3_3 = h\n",
    "        h = self.slice4(h)\n",
    "        h_relu4_3 = h\n",
    "        h = self.slice5(h)\n",
    "        h_relu5_3 = h\n",
    "        vgg_outputs = namedtuple(\n",
    "            \"VggOutputs\", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3'])\n",
    "        out = vgg_outputs(h_relu1_2, h_relu2_2,\n",
    "                          h_relu3_3, h_relu4_3, h_relu5_3)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18eb643",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LPIPS_VGG(nn.Module):\n",
    "    def __init__(self, pretrained=True, net='alex', version='0.1', lpips=True, spatial=False,\n",
    "                 pnet_rand=False, pnet_tune=False, use_dropout=True, model_path=None, eval_mode=True, verbose=True):\n",
    "        # lpips - [True] means with linear calibration on top of base network\n",
    "        # pretrained - [True] means load linear weights\n",
    "\n",
    "        super(LPIPS, self).__init__()\n",
    "\n",
    "        self.pnet_type = net\n",
    "        self.pnet_tune = pnet_tune\n",
    "        self.pnet_rand = pnet_rand\n",
    "        self.spatial = spatial\n",
    "        self.lpips = lpips  # false means baseline of just averaging all layers\n",
    "        self.version = version\n",
    "        self.scaling_layer = ScalingLayer()\n",
    "\n",
    "        if self.pnet_type in ['vgg', 'vgg16']:\n",
    "            net_type = pn.vgg16\n",
    "            self.chns = [64, 128, 256, 512, 512]\n",
    "        elif self.pnet_type == 'alex':\n",
    "            net_type = pn.alexnet\n",
    "            self.chns = [64, 192, 384, 256, 256]\n",
    "        elif self.pnet_type == 'squeeze':\n",
    "            net_type = pn.squeezenet\n",
    "            self.chns = [64, 128, 256, 384, 384, 512, 512]\n",
    "        self.L = len(self.chns)\n",
    "\n",
    "        self.net = net_type(pretrained=not self.pnet_rand,\n",
    "                            requires_grad=self.pnet_tune)\n",
    "\n",
    "        if lpips:\n",
    "            self.lin0 = NetLinLayer(self.chns[0], use_dropout=use_dropout)\n",
    "            self.lin1 = NetLinLayer(self.chns[1], use_dropout=use_dropout)\n",
    "            self.lin2 = NetLinLayer(self.chns[2], use_dropout=use_dropout)\n",
    "            self.lin3 = NetLinLayer(self.chns[3], use_dropout=use_dropout)\n",
    "            self.lin4 = NetLinLayer(self.chns[4], use_dropout=use_dropout)\n",
    "            self.lins = [self.lin0, self.lin1, self.lin2, self.lin3, self.lin4]\n",
    "            if self.pnet_type == 'squeeze':  # 7 layers for squeezenet\n",
    "                self.lin5 = NetLinLayer(self.chns[5], use_dropout=use_dropout)\n",
    "                self.lin6 = NetLinLayer(self.chns[6], use_dropout=use_dropout)\n",
    "                self.lins += [self.lin5, self.lin6]\n",
    "            self.lins = nn.ModuleList(self.lins)\n",
    "\n",
    "            if pretrained:\n",
    "                if model_path is None:\n",
    "                    import inspect\n",
    "                    import os\n",
    "                    model_path = os.path.abspath(os.path.join(inspect.getfile(\n",
    "                        self.__init__), '..', 'weights/v%s/%s.pth' % (version, net)))\n",
    "\n",
    "                if verbose:\n",
    "                    print('Loading model from: %s' % model_path)\n",
    "                self.load_state_dict(torch.load(\n",
    "                    model_path, map_location='cpu'), strict=False)\n",
    "\n",
    "        if eval_mode:\n",
    "            self.eval()\n",
    "\n",
    "    def forward(self, in0, in1, retPerLayer=False, normalize=False):\n",
    "        # turn on this flag if input is [0,1] so it can be adjusted to [-1, +1]\n",
    "        if normalize:\n",
    "            in0 = 2 * in0 - 1\n",
    "            in1 = 2 * in1 - 1\n",
    "\n",
    "        # v0.0 - original release had a bug, where input was not scaled\n",
    "        in0_input, in1_input = (self.scaling_layer(in0), self.scaling_layer(\n",
    "            in1)) if self.version == '0.1' else (in0, in1)\n",
    "        outs0, outs1 = self.net.forward(in0_input), self.net.forward(in1_input)\n",
    "        feats0, feats1, diffs = {}, {}, {}\n",
    "\n",
    "        for kk in range(self.L):\n",
    "            feats0[kk], feats1[kk] = lpips.normalize_tensor(\n",
    "                outs0[kk]), lpips.normalize_tensor(outs1[kk])\n",
    "            diffs[kk] = (feats0[kk] - feats1[kk]) ** 2\n",
    "\n",
    "        if self.lpips:\n",
    "            if self.spatial:\n",
    "                res = [upsample(self.lins[kk](diffs[kk]), out_HW=in0.shape[2:])\n",
    "                       for kk in range(self.L)]\n",
    "            else:\n",
    "                res = [spatial_average(self.lins[kk](\n",
    "                    diffs[kk]), keepdim=True) for kk in range(self.L)]\n",
    "        else:\n",
    "            if self.spatial:\n",
    "                res = [upsample(diffs[kk].sum(dim=1, keepdim=True),\n",
    "                                out_HW=in0.shape[2:]) for kk in range(self.L)]\n",
    "            else:\n",
    "                res = [spatial_average(diffs[kk].sum(\n",
    "                    dim=1, keepdim=True), keepdim=True) for kk in range(self.L)]\n",
    "\n",
    "        val = res[0]\n",
    "        for l in range(1, self.L):\n",
    "            val += res[l]\n",
    "\n",
    "        # a = spatial_average(self.lins[kk](diffs[kk]), keepdim=True)\n",
    "        # b = torch.max(self.lins[kk](feats0[kk]**2))\n",
    "        # for kk in range(self.L):\n",
    "        #     a += spatial_average(self.lins[kk](diffs[kk]), keepdim=True)\n",
    "        #     b = torch.max(b,torch.max(self.lins[kk](feats0[kk]**2)))\n",
    "        # a = a/self.L\n",
    "        # from IPython import embed\n",
    "        # embed()\n",
    "        # return 10*torch.log10(b/a)\n",
    "\n",
    "        if retPerLayer:\n",
    "            return val, res\n",
    "        else:\n",
    "            return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b21b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lpips]",
   "language": "python",
   "name": "conda-env-lpips-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
